# ğŸ¤– gh-chat-dataset: Turn Any GitHub Repo into AI Training Data

**One-click tool to convert GitHub repositories into high-quality chat datasets for fine-tuning language models like Qwen with MLX.**

> ğŸ’¡ **Perfect for**: Training coding assistants, documentation bots, or domain-specific AI models from real codebases.

## ğŸ¯ What Does This Do?

This tool automatically extracts meaningful code-documentation pairs from GitHub repositories and formats them as conversation data that AI models can learn from. 

**Think of it as**: Taking a repository full of Python functions with docstrings, JavaScript with JSDoc comments, and README files, then converting them into "teacher-student" conversations for AI training.

### ğŸ§  The Magic Behind It

- **Python Functions** â†’ "Write a docstring for this function" conversations
- **JavaScript/TypeScript** â†’ "Add JSDoc comments to this code" examples  
- **Markdown Documentation** â†’ "Explain this section" Q&A pairs
- **Smart Processing** â†’ Removes duplicates, filters by length, splits for training

## ğŸš€ Quick Start (3 Minutes)

### Step 1: Install
```bash
# Clone this repository
git clone https://github.com/ArjunDivecha/Repo2Dataset.git
cd Repo2Dataset

# Install the tool
pip install -e .[dev]
```

### Step 2: Convert a Repository
```bash
# Example: Convert the popular 'requests' library into training data
gh-chat-dataset --repo https://github.com/psf/requests.git --out ./requests_dataset

# Or try a smaller example first
gh-chat-dataset --repo https://github.com/pallets/itsdangerous.git --out ./test_dataset
```

### Step 3: Check Your Results
```bash
ls test_dataset/
# You'll see:
# dataset.train.jsonl  <- 90% of samples for training
# dataset.valid.jsonl  <- 10% of samples for validation  
# stats.json          <- Summary of what was extracted
```

## ğŸ“Š Real Example Output

Let's say you run it on a Python repository. Here's what one training sample looks like:

**Input** (what the AI sees):
```
Write a clear, concise docstring for this function:

def validate_email(email):
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}
    return re.match(pattern, email) is not None
```

**Expected Output** (what the AI should respond):
```
Validate if an email address has a proper format.

Args:
    email (str): The email address to validate.

Returns:
    bool: True if email format is valid, False otherwise.
```

This creates training data that teaches AI models to write good documentation!

## âš™ï¸ All Command Options Explained

```bash
gh-chat-dataset [OPTIONS]
```

### Required Options
- `--repo URL` - The GitHub repository to convert (must be public or you need access)
- `--out DIRECTORY` - Where to save the dataset files

### Optional Fine-Tuning
- `--max-tokens 2048` - Skip samples longer than this (prevents memory issues)
- `--split-ratio 0.9` - How much data goes to training vs validation (0.9 = 90% train, 10% validation)
- `--allow-llm` - *(Experimental)* Use AI to generate labels where missing

### Real Examples
```bash
# Basic usage
gh-chat-dataset --repo https://github.com/user/repo.git --out ./my_dataset

# For larger models (more context)
gh-chat-dataset --repo https://github.com/user/repo.git --out ./my_dataset --max-tokens 4096

# More data for validation
gh-chat-dataset --repo https://github.com/user/repo.git --out ./my_dataset --split-ratio 0.8
```

## ğŸ“ What Gets Extracted?

### From Python Files (.py)
- âœ… Functions with docstrings â†’ "Write docstring" examples
- âœ… Classes with docstrings â†’ "Document this class" examples  
- âœ… Module docstrings â†’ "Summarize this module" examples

### From JavaScript/TypeScript (.js, .jsx, .ts, .tsx)
- âœ… Functions with JSDoc comments â†’ "Add JSDoc" examples
- âœ… Complex function signatures â†’ Documentation examples

### From Markdown Files (.md)
- âœ… README sections â†’ "Explain this concept" Q&A
- âœ… Documentation pages â†’ Knowledge Q&A pairs
- âœ… API docs â†’ Usage explanation examples

### What Gets Filtered Out
- âŒ Files without documentation (can't create good training pairs)
- âŒ Very short or very long samples (poor quality for training)
- âŒ Duplicate content (prevents overfitting)
- âŒ Generated files (node_modules, build outputs, etc.)

## ğŸ“ˆ Understanding Your Output

After running the tool, check `stats.json`:

```json
{
  "sha": "abc123...",           // Exact version of repo used
  "counts": {
    "total": 156,               // Total samples created
    "train": 140,               // Training samples (90%)
    "valid": 16                 // Validation samples (10%)
  }
}
```

**Good numbers to see:**
- 50+ total samples for small projects
- 500+ total samples for substantial codebases
- 1000+ total samples for large, well-documented projects

## ğŸ¯ Perfect Repositories to Try

### Great for Beginners
- `pallets/itsdangerous` - Small, well-documented Python library
- `sindresorhus/is` - Simple JavaScript utilities with good docs
- `getsentry/sentry-python` - Medium-sized Python project

### For Larger Datasets  
- `psf/requests` - Popular Python HTTP library
- `microsoft/TypeScript` - Large TypeScript codebase
- `django/django` - Web framework with extensive docs

### Best Results Come From
- âœ… Well-documented codebases
- âœ… Projects with consistent docstring/JSDoc style
- âœ… Repositories with good README files
- âœ… Active projects (recent commits)

## ğŸ›  Using Your Dataset with MLX

Once you have your dataset, here's how to use it for fine-tuning:

```python
# Load your dataset
import json

def load_dataset(path):
    with open(path, 'r') as f:
        return [json.loads(line) for line in f]

train_data = load_dataset('your_dataset/dataset.train.jsonl')
valid_data = load_dataset('your_dataset/dataset.valid.jsonl')

# Each sample has this format:
sample = train_data[0]
print(sample['messages'])  # The conversation
print(sample['meta'])      # Metadata about source
```

**Pro Tip**: The `messages` format is compatible with most modern fine-tuning frameworks including MLX, transformers, and OpenAI's fine-tuning API.

## ğŸ”§ Troubleshooting

### "No samples extracted"
- âœ… Make sure the repo has documented code (docstrings, JSDoc, README)
- âœ… Try a well-known repo first (like `pallets/itsdangerous`)
- âœ… Check if the repo is public or you have access

### "Permission denied" errors
- âœ… Make sure you can access the repository
- âœ… For private repos, ensure your Git credentials are set up
- âœ… Try with a public repository first

### "Out of memory" errors  
- âœ… Use `--max-tokens 1024` for smaller samples
- âœ… Try processing smaller repositories first
- âœ… Make sure you have sufficient disk space

### Dataset is too small
- âœ… Try repositories with more documentation
- âœ… Look for projects with consistent docstring styles
- âœ… Consider enabling `--allow-llm` for more samples (experimental)

## ğŸš€ Advanced Usage

### Batch Processing Multiple Repos
```bash
# Create a script to process multiple repositories
repos=(
  "https://github.com/user/repo1.git"
  "https://github.com/user/repo2.git" 
  "https://github.com/user/repo3.git"
)

for repo in "${repos[@]}"; do
  name=$(basename "$repo" .git)
  gh-chat-dataset --repo "$repo" --out "./datasets/$name"
done
```

### Combining Datasets
```bash
# Merge multiple datasets
cat dataset1/dataset.train.jsonl dataset2/dataset.train.jsonl > combined_train.jsonl
cat dataset1/dataset.valid.jsonl dataset2/dataset.valid.jsonl > combined_valid.jsonl
```

## ğŸ‘¨â€ğŸ’» Contributing & Development

Want to improve the tool? Here's how to set up for development:

```bash
# Clone and setup
git clone https://github.com/ArjunDivecha/Repo2Dataset.git
cd Repo2Dataset

# Install in development mode
pip install -e .[dev]

# Run tests to make sure everything works
pytest

# Check code style
ruff check .

# Make your changes, then test
python -m pytest
```

### Adding New File Types
The tool is designed to be extensible. To add support for new languages:

1. Create a new extractor in `gh_chat_dataset/extract_xxx.py`
2. Add a builder function in `gh_chat_dataset/builders.py`
3. Update the file discovery patterns in `gh_chat_dataset/discover.py`
4. Add tests in `tests/`

## ğŸ“„ License

MIT License - feel free to use this for any project!

## ğŸ™‹â€â™€ï¸ Questions?

- ğŸ“– Check the troubleshooting section above
- ğŸ› Found a bug? [Open an issue](https://github.com/ArjunDivecha/Repo2Dataset/issues)
- ğŸ’¡ Have an idea? [Start a discussion](https://github.com/ArjunDivecha/Repo2Dataset/discussions)

---

**Happy Training! ğŸ¤–âœ¨**