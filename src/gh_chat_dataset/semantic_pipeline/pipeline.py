"""
=============================================================================
SCRIPT NAME: pipeline.py
=============================================================================

INPUT FILES:
- GitHub repository working tree cloned locally for analysis.

OUTPUT FILES:
- Semantic dataset JSONL files generated by `SemanticWriter` (subdirectory of out path).

VERSION HISTORY:
- v1.0 (2025-09-28): Initial orchestration of semantic pipeline stages.

LAST UPDATED: 2025-09-28

NOTES:
- Orchestrates parsing, tagging, embedding, clustering, LLM synthesis, and serialization.
- Requires API keys for OpenAI and Anthropic to be set in environment variables.
=============================================================================
"""

from __future__ import annotations

from pathlib import Path
from typing import List

from ..semantic_types import ConversationRecord, ParsedDocument, Span
from .cluster import ClusteringConfig, SemanticClusterer
from .embedder import OpenAIEmbedder
from .ontology import OntologyTagger
from .parser import parse_repository
from .synthesizer import SemanticSynthesizer
from .writer import SemanticWriter


class SemanticPipeline:
    def __init__(
        self,
        cache_dir: Path | None = None,
        claude_model: str = "anthropic.claude-opus-4-6-v1",
        openai_model: str = "gpt-4.1-mini",
        clustering_config: ClusteringConfig | None = None,
    ) -> None:
        self.cache_dir = cache_dir
        self.tagger = OntologyTagger.default()
        self.embedder = OpenAIEmbedder(cache_dir=cache_dir)
        self.clusterer = SemanticClusterer(config=clustering_config)
        self.synthesizer = SemanticSynthesizer(
            claude_model=claude_model,
            openai_model=openai_model,
        )

    def run(self, repo_path: Path, output_dir: Path) -> None:
        documents = parse_repository(repo_path)
        spans = self._collect_spans(documents)
        self.tagger.tag(spans)
        embeddings = self.embedder.embed_spans(spans)
        clusters = self.clusterer.cluster(spans, embeddings)
        conversations = self.synthesizer.generate(clusters)
        writer = SemanticWriter(output_dir)
        writer.write(conversations)

    def _collect_spans(self, documents: List[ParsedDocument]) -> List[Span]:
        spans: List[Span] = []
        for doc in documents:
            spans.extend(doc.spans)
        return spans


def run_semantic_pipeline(
    repo_path: Path,
    output_dir: Path,
    cache_dir: Path | None = None,
    claude_model: str = "anthropic.claude-opus-4-6-v1",
    openai_model: str = "gpt-4.1-mini",
    clustering_config: ClusteringConfig | None = None,
) -> None:
    pipeline = SemanticPipeline(
        cache_dir=cache_dir,
        claude_model=claude_model,
        openai_model=openai_model,
        clustering_config=clustering_config,
    )
    pipeline.run(repo_path=repo_path, output_dir=output_dir)
